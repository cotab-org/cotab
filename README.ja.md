# Cotab

[English](README.md) | [日本語](README.ja.md) | [中文](README.zh-cn.md)

このVS Code拡張機能は、完全にローカルLLM上で動作することで、最大限のプライバシーとセキュリティを考慮して設計されたAI駆動のマルチライン自動補完プラグインです。

ファイル全体のコンテキストだけでなく、外部シンボル、エラー、以前の編集履歴に基づいてAIを使用して複数行のコードを生成し、自動補完の提案として表示します。
特にQwen3-Coder-30B-A3Bはクラウドサービスに匹敵するほどの高品質な補完を提供し、VRAM 8GBの環境でもワンクリックでセットアップできるように最適化されています。

翻訳専用モードも提供しています。

## 自動補完
![Autocomplete Demo](doc/asset/cotab-tutorial-autocomplete1.gif)
サポートされるプログラミング言語は、AIモデルによって異なります。デフォルトモデルのQwen3-4B-Instruct-2507はコンパクトなサイズにもかかわらず、多くの言語をサポートしています。

## 自動コメントモード
![Comment Demo](doc/github-asset/cotab-demo-comment.gif)
コードにコメントを追加する専用モードです。AIがカーソル位置のコードを理解し、詳細なコメントを自動的に追加します。
Qwen3-4B-Instruct-2507でも十分に良いコメントを提供しますが、この用途ではパフォーマンス低下を許容できるため、最高の結果を得るためにQwen3-Coder-30B-A3Bを使用を推奨します。

## 自動翻訳モード
![Translate Demo](doc/github-asset/cotab-demo-translate.gif)
翻訳専用モードです。不慣れな言語のコメントを自動的に翻訳し、コードを理解しやすくします。
Qwen3-4B-Instruct-2507も高品質な翻訳を提供しますが、この用途ではパフォーマンス低下を許容できるため、最高の結果を得るためにQwen3-Coder-30B-A3Bを使用を推奨します。

## 機能
- プライバシーを優先し、ローカルLLMを使用して完全にオフラインで動作
- インライン提案に焦点を当てた機能を提供
- カーソル位置からのインライン補完だけでなく、マルチライン編集も提案
- ターゲットファイルの全コンテンツ、他のファイルからのシンボル、編集履歴を考慮した提案を提供
- llama-serverに最適化された高速レスポンスを提供
- 自動コメントと自動翻訳のモードも提供
- 透明性を確保するオープンソース

## はじめに
1. VS CodeマーケットプレイスからCotabをインストール
   ![Getting started - install](doc/github-asset/cotab-demo-install.gif)
  
2. "Install Local Server"ボタンをクリックするか、APIを設定します。
   ![Getting started - setup](doc/github-asset/cotab-demo-setup.gif)
   備考:
   - 初回は2.5GBのモデルをダウンロードするため、時間がかかる場合があります。
   - インストール対応プラットフォーム: Windows/MacOS/Ubuntu
  
3. 入力開始！
   ![Getting started - completion](doc/github-asset/cotab-demo-completion.gif)
   
   |コマンド|キーバインド|
   | ---- | ---- |
   |すべて受け入れ|Tab|
   |最初の行を受け入れ|Shift + Tab|
   |拒否|Esc|

   備考:
   - 拒否することで、次の補完候補を変更できます。
   - イタリック表示のオーバーレイは、AIがまだ結果を出力中で、結果が確定していないことを意味します。ほとんどの場合、最終結果と同じですが、イタリック表示の場合、マージ結果に問題がある可能性があります。

## 重要な注意事項
- リクエストには通常、10,000トークンを超えるプロンプトが含まれます
- llama-serverに最適化されています。llama-serverの使用を強く推奨します
- **従量課金型APIサーバーを使用する場合は特に注意してください。トークン消費が急速に増加する可能性があります**
- ローカルサーバーを使用する場合、**シングルユーザー使用を強く推奨します**

  ローカルサーバーは**シングルユーザー**に最適化されています。
  複数のユーザーが同時に使用すると、推論が大幅に遅延し、レスポンス速度が著しく低下します。

## 使用のヒント

- コメントを先に書く

  デフォルトモデル（Qwen3-4B-Instruct-2507）はコンパクトでありながら非常に高性能ですが、コード補完専用に設計されているわけではありません。最近の多くのクラウドサービスとは異なり、書きたいコードをすぐに提案しない場合があります。そのような場合、まず書きたいコードを説明するコメントを書くことで、モデルが説明に基づいてより正確なコード提案を生成できるようになります。
  ![comment first](doc/github-asset/cotab-demo-comment-first.gif)
  
- プロンプトを編集する

  モデルの品質は重要ですが、補完の精度はプロンプトの内容によって大きく変わります。プロンプトをカスタマイズすることで、精度をさらに向上出来る可能性があります。

  また、独自のカスタムモードを作成することもできます。
  
  プロンプトを編集するには、メニューから開きます。デフォルトのプロンプトはコメントアウトされています。コメントを解除し、編集して保存すると、変更が補完にすぐに反映されます。
  ![open prompt](doc/github-asset/cotab-demo-open-prompt.gif)

## パフォーマンス
- **推奨:** 最適なパフォーマンスのために、GeForce RTX 3000シリーズ以降のGPU（または同等品）。

- Cotabはllama-serverとQwen3-4B-Instruct-2507に最適化されており、高速動作を目的に設計されています。2回目以降のリクエストでは、1,000行を超えるソースファイルでも、プロンプトが15,000トークンを超え、数百の参照シンボルが含まれていても、GeForce RTX 4070では約0.5秒で全コンテキストを理解し、補完を表示します。その後も、キーストロークごとに補完リクエストを送信し続け、維持します。

- AI処理は、GeForce RTX 3000シリーズ以降で大幅なパフォーマンス向上が見られます。快適なレスポンスには、GeForce RTX 3000シリーズ以降のGPUまたは同等品を推奨します。

## 詳細
- llama-server

  OpenAI互換APIが使用できますが、llama-serverの使用を強く推奨します。llama-serverはオーバーヘッドが低く、llama.cppをバックエンドとして使用するサーバーの中で最高速度で動作します。
  コード補完は頻繁にリクエストとキャンセルを繰り返すため、そのオーバーヘッドがユーザー体験に直接影響します。
  
- プロンプト最適化

  llama-serverには、以前のリクエストからのプロンプトをキャッシュするメカニズムがデフォルトで有効になっています。プロンプトキャッシュは、以前のプロンプトと一致する部分まで有効で、その部分までのプロンプト処理をスキップできます。
  
  このメカニズムを最大限に活用するため、プロンプト内の元のソースコードは、ユーザーが入力しても変更されません。代わりに、変更された周辺コードの最小限のブロックがプロンプトの下部に追加されます。

  プロンプトは完全にカスタマイズ可能で、準備されたモードをワンクリックで切り替えることができます。
  これにより、目的に応じた最適なプロンプトで補完を実行できます。
  
- 編集履歴

  ユーザーの直前の編集を記憶し、提案に活用します。編集は追加、削除、編集、名前変更、コピーに分類され、予測精度が向上します。
  
  これにより、直前に作成された関数が提案されやすくなり、ユーザーの意図をより正確に反映します。
  
- 他のファイルからのシンボル

  VSCodeの言語プロバイダーから取得可能なシンボルを使用し、提案に活用します。これらのシンボルにより、LLMがクラス構造を理解し、メンバー関数の提案の精度が向上します。

  注意: シンボルは、VS Codeで表示されたファイルの順序で入力されます。

- エラー問題

  診断エラーを入力として使用し、エラーを修正するコードを生成します。
  小さなAIモデルでも、エラーを修正することを学習するため、提案の品質がさらに向上します。
  
- コード要約

  ソースコードを事前に要約し、結果をプロンプトに組み込むことで、より深いレベルの理解を可能にします。
  この機能はデフォルトで無効になっています。全コードが入力されるため、要約がなくても補完の品質が保証されているためです。

- 進捗アイコンの説明

  |アイコン|説明|
  | ---- | ---- |
  |![spinner dot](doc/github-asset/readme-dot-spinner-0.png)|ソースコードを分析中|
  |![spinner red](doc/github-asset/readme-spinner-red-0.png)|現在の行を補完中|
  |![spinner normal](doc/github-asset/readme-spinner-0.png)|現在の行以降を補完中|
  
## プライバシーとテレメトリー
- Cotabは、デフォルトのエンドポイント`"http://localhost:8080/v1"`またはユーザーが指定したLLM APIとのみ通信します。他の外部サービスやサーバーには接続しません。これにより、最大限のプライバシーとセキュリティが確保されます。
  - 設定されたAPIとのみ通信
  - テレメトリーや使用データは一切送信されません
  - ユーザーのコードや入力が第三者と共有されることはありません
  - 個人情報は収集または保存されません
  - このプロジェクトはオープンソースであり、すべてのソースコードがGitHubで利用可能です

- このポリシーにより、Cotabを完全に安心して使用できます。
- 注意: ローカルサーバーをインストールする場合、[llama.cpp githubリポジトリ](https://github.com/ggml-org/llama.cpp/releases)にアクセスします。

## 開発 / 貢献

- 貢献（イシュー、PR、改善提案）を歓迎します。
- バグ修正、最適化、ベンチマーク結果の共有も歓迎します。

## ビルド方法

- セットアップ要件

  事前にVS Codeをインストールしてください。

- Windows

  この単一コマンドを実行すると、セットアップスクリプトが自動的にダウンロードされ実行されます。GitやNode.jsを含む何も必要ありません - すべてのポータブル版が自動的にダウンロードされ、./workspaceにセットアップされ、プロジェクトがクローンされ、VS Codeが起動します:
  
  ```bash
  mkdir cotab
  cd cotab
  powershell -NoProfile -Command "$f='run-vscode.bat'; (New-Object Net.WebClient).DownloadString('https://github.com/cotab-org/cotab/raw/refs/heads/main/run-vscode.bat') -replace \"`r?`n\",\"`r`n\" | Set-Content $f -Encoding ASCII; cmd /c $f"
  ```
  
  vscodeでF5を押してプラグインのデバッグを開始します。
  
- Ubuntu

  Node.js(v22)が必要です。
    
  例: パッケージマネージャー経由でNode.js v22をインストール。
  ```bash
  url -fsSL https://deb.nodesource.com/setup_22.x | sudo -E bash -
  sudo apt install -y nodejs
  ```
  
  Cotabのクローンと設定。
  
  ```bash
  git clone https://github.com/cotab-org/cotab.git
  cd cotab
  npm install
  code .\
  ```
  
  vscodeでF5を押してプラグインのデバッグを開始します。
  
- MacOS

  Node.js(v22)が必要です。
  
  例: macosでNode.js v22をインストール。
  ```bash
  # nvmをインストール
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
  
  # nvmを有効化
  \. "$HOME/.nvm/nvm.sh"
  
  # node.js v22をインストール
  nvm install 22
  node -v
  ```
  
  Cotabのクローンと設定。
  
  ```bash
  git clone https://github.com/cotab-org/cotab.git
  cd cotab
  npm install
  code .\
  ```
  
  vscodeでF5を押してプラグインのデバッグを開始します。

- パッケージの作成

```bash
npx vsce package
```

## FAQ

### Cotabの使用を開始する際に、ウィンドウが一瞬ちらつくのはなぜですか？
一瞬のウィンドウのちらつきは、Cotabが初期化中にフォントサイズを計算するために発生します。VS Codeは文字サイズを取得する直接的なAPIを提供していないため、CotabはWebviewを使用してフォントサイズを計算します。これにより、Cotabの使用を開始する際に一瞬のちらつきが発生します。

## ライセンス
Copyright (c) 2025-2026 cotab
Apache License 2.0

