# Cotab

[English](README.md) | [日本語](README.ja.md)

このVS Code拡張機能は、完全にローカルLLM上で動作することで、最大限のプライバシーとセキュリティを考慮して設計されたAI駆動のマルチライン自動補完プラグインです。コンシューマグレードのPCで動作することを念頭に開発されています。

Cotabは編集の意図を加味したオートコンプリートに重点を置いています。ファイル全体のコンテキストに加えて、外部シンボル、エラー、以前の編集履歴を考慮し、AIが複数行のコードを生成し、自動補完の提案として表示します。
**セットアップはワンクリックで完了し、すぐに利用を開始できます。**また、ワンクリックで使用モデルを切り替え可能で、Qwen3-Coder-30B-A3BはVRAM 4GBや8GB環境でも動作します。

[クイックスタート](#クイックスタート) | [質問 / アイデア / フィードバック](https://github.com/cotab-org/cotab/discussions) (English / 日本語OK)

## 自動補完
![Autocomplete Demo](doc/asset/cotab-tutorial-autocomplete1.gif)
サポートされるプログラミング言語は、AIモデルによって異なります。デフォルトモデルのQwen3-4B-Instruct-2507は、コンパクトなサイズにもかかわらず多くの言語をサポートしています。
Qwen3-Coder-30B-A3Bはさらに多くの言語をサポートし、まるで魔法のように、まさに入力しようとしているコードを提案します。

## 自動コメントモード
![Comment Demo](doc/github-asset/cotab-demo-comment.gif)
コードにコメントを追加する専用モードです。通常のモードよりも深くコードを分析し、アルゴリズムの意図を理解した上で、詳細なコメントを自動的に追加します。
Qwen3-4B-Instruct-2507でも十分に良いコメントを提供しますが、この用途ではパフォーマンス低下を許容できるため、最高の結果を得るためにQwen3-Coder-30B-A3Bを使用を推奨します。

## 自動翻訳モード
![Translate Demo](doc/github-asset/cotab-demo-translate.gif)
翻訳専用モードです。コードコメントだけでなく、通常のテキストファイルも翻訳できます。
Qwen3-4B-Instruct-2507も高品質な翻訳を提供しますが、この用途ではパフォーマンス低下を許容できるため、最高の結果を得るためにQwen3-Coder-30B-A3Bを使用を推奨します。

## 機能
- プライバシーを優先し、ローカルLLMを使用して完全にオフラインで動作
- インライン提案に焦点を当てた機能を提供
- カーソル位置からのインライン補完だけでなく、マルチライン編集も提案
- ターゲットファイルの全コンテンツ、他のファイルからのシンボル、編集履歴を考慮した提案を提供
- llama-serverに最適化された高速レスポンスを提供
- 自動コメントと自動翻訳のモードも提供
- 透明性を確保するオープンソース

## クイックスタート
1. [VS Codeマーケットプレイス](https://marketplace.visualstudio.com/items?itemName=cotab.cotab)からCotabをインストール
   ![Getting started - install](doc/github-asset/cotab-demo-install.gif)
  
2. "Install Local Server"ボタンをクリックするか、APIを設定します。
   ![Getting started - setup](doc/github-asset/cotab-demo-setup.gif)
   備考：
   - 初回は2.5GBのモデルをダウンロードするため、時間がかかる場合があります。
   - インストール後自動でサーバーが起動します。
   - インストール対応プラットフォーム: Windows/MacOS/Ubuntu
  
3. 入力開始！
   ![Getting started - completion](doc/github-asset/cotab-demo-completion.gif)
   
   |コマンド|キーバインド|
   | ---- | ---- |
   |すべて受け入れ|Tab|
   |最初の行を受け入れ|Shift + Tab|
   |拒否|Esc|

   備考：
   - 拒否することで、次の補完候補を変更できます。
   - イタリック表示のオーバーレイは、AIがまだ結果を出力中で、結果が確定していないことを意味します。ほとんどの場合、最終結果と同じですが、イタリック表示の場合、マージ結果に問題がある可能性があります。

## 重要な注意事項
- リクエストには通常、10,000トークンを超えるプロンプトが含まれます
- llama-serverに最適化されています。llama-serverの使用を強く推奨します
- **従量課金型APIサーバーを使用する場合は特に注意してください。トークン消費が急速に増加する可能性があります**
- ローカルサーバーを使用する場合、**シングルユーザー使用を強く推奨します**

  ローカルサーバーは**シングルユーザー**に最適化されています。
  複数のユーザーが同時に使用すると、推論が大幅に遅延し、レスポンス速度が著しく低下します。

## 使用のヒント

- コメントを先に書く

  デフォルトモデル（Qwen3-4B-Instruct-2507）はコンパクトでありながら非常に高性能ですが、コード補完専用に設計されているわけではありません。最近の多くのクラウドサービスとは異なり、書きたいコードをすぐに提案しない場合があります。そのような場合、まず書きたいコードを説明するコメントを書くことで、モデルが説明に基づいてより正確なコード提案を生成できるようになります。
  ![comment first](doc/github-asset/cotab-demo-comment-first.gif)
  
- プロンプトを編集する

  モデルの品質は重要ですが、補完の精度はプロンプトの内容によって大きく変わります。プロンプトをカスタマイズすることで、精度をさらに向上できる可能性があります。

  また、独自のカスタムモードを作成することもできます。
  
  プロンプトを編集するには、メニューから開きます。デフォルトのプロンプトはコメントアウトされています。コメントを解除し、編集して保存すると、変更が補完にすぐに反映されます。
  ![open prompt](doc/github-asset/cotab-demo-open-prompt.gif)

## パフォーマンス
- **推奨:** 最適なパフォーマンスのために、GeForce RTX 3000シリーズ以降のGPU（または同等品）。

- Cotabはllama-serverとQwen3-4B-Instruct-2507に最適化されており、高速動作を目的に設計されています。1,000行を超えるソースコードで、数百の外部参照シンボルが含まれる実践的な環境では、プロンプトは15,000トークンを超えます。そのような状況でも全コンテキストを理解し、GeForce RTX 4070では2回目以降の補完は約0.5秒で表示されます。

- AI処理は、GeForce RTX 3000シリーズ以降で性能が大幅に向上しています。快適なレスポンスには、GeForce RTX 3000シリーズ以降のGPUまたは同等品をお勧めします。

## 詳細
- llama-server

  OpenAI互換APIが使用できますが、llama-serverの使用を強く推奨します。llama-serverはオーバーヘッドが低く、llama.cppをバックエンドとして使用するサーバーの中でも高速に動作します。[詳細はこちら](#リモートサーバーの利用)
  
- プロンプト最適化

  llama-serverには、以前のリクエストからのプロンプトをキャッシュするメカニズムがデフォルトで有効になっています。プロンプトキャッシュは、以前のプロンプトと一致する部分まで有効で、その部分までのプロンプト処理をスキップできます。
  
  このメカニズムを最大限に活用するため、プロンプト内の元のソースコードは、ユーザーが入力しても変更されません。代わりに、変更された周辺コードの最小限のブロックがプロンプトの下部に追加されます。

  プロンプトは完全にカスタマイズ可能で、準備されたモードをワンクリックで切り替えることができます。
  これにより、目的に応じた最適なプロンプトで補完を実行できます。
  
- 編集履歴

  ユーザーの直前の編集を記憶し、提案に活用します。編集は追加、削除、編集、名前変更、コピーに分類され、予測精度が向上します。
  
  これにより、直前に作成された関数が提案されやすくなり、ユーザーの意図をより正確に反映します。
  
- 他のファイルからのシンボル

  VSCodeの言語プロバイダーから取得可能なシンボルを使用し、提案に活用します。これらのシンボルにより、LLMがクラス構造を理解し、メンバー関数の提案の精度が向上します。

  注意：シンボルは、VS Codeで表示されたファイルの順序で入力されます。

- エラー問題

  診断エラーを入力として使用し、エラーを修正するコードを生成します。
  小さなAIモデルでも、エラーを修正することを学習するため、提案の品質がさらに向上します。
  
- コード要約

  ソースコードを事前に要約し、結果をプロンプトに組み込むことで、より深いレベルの理解を可能にします。
  この機能はデフォルトで無効になっています。全コードが入力されるため、要約がなくても補完の品質が保証されているためです。

- 進捗アイコンの説明

  |アイコン|説明|
  | ---- | ---- |
  |![spinner dot](doc/github-asset/readme-dot-spinner-0.png)|ソースコードを分析中|
  |![spinner red](doc/github-asset/readme-spinner-red-0.png)|現在の行を補完中|
  |![spinner normal](doc/github-asset/readme-spinner-0.png)|現在の行以降を補完中|
  
## 利用モデルについて
すべてのテキスト生成モデルが利用可能ですが、局所的なコード生成を行うために強力な指示追従性能が必要です。（指示追従性能とは、プロンプトのルールを厳密に守り、ルール違反をしない性能のことです。）

- Qwen3-Coder-30B-A3B

  モデルの8割がコード学習に割り当てられており、高品質の補完を提供します。実際の演算は3B相当のため、小型モデルのように高速で動作します。このモデルはパフォーマンスを著しく低下させることなくVRAM使用率を調整できるため、Cotabでは4GBや8GB環境でも動作するプリセットを用意しています。

- Qwen3-4B-Instruct-2507

  4Bという非常に小さいサイズながら、ずば抜けた指示追従性能と、数学などの分野における高い性能があります。Cotabにおいてもその小ささを感じさせないほどの良い補完を提供します。

- Ministral-3-3B-Instruct-2512

  3Bという非常に小さいサイズながら、高い性能があり、VRAM 5GBの使用量で高速に動作します。VRAMに制限がある場合はお試しください。

- granite-4.0-micro

  Cotabにおいてはしばしば壊れた補完を生成するため、非推奨となっています。

- LFM2-2.6B

  わずかVRAM 3GBの使用量で、Qwen3-4B-Instruct-2507よりも倍速で動作するように設計されています。
  Cotabのコード補完においてはコードが壊れる問題が散見されたため、非推奨となっています。
  しかし、VRAM要件が非常に厳しい場合や翻訳用途に利用できます。

## リモートサーバーの利用

OpenAI互換APIサーバーを利用することができますが、パフォーマンス上の理由から**llama-server**か**llama-swap**を利用することを強く推奨します。
特に**llama-swap**を通して**llama-server**を利用することで、他のチャットプラグイン利用時に自動でモデルを切り替えることができます。

- **最重要事項**

  - **llama-serverを使う場合必ず"-np 1"オプションを指定してください。**
    llama-serverの2025年末の更新により、デフォルトで4並列で動作するように変更されています。デフォルトのllama-serverの場合、Cotabでは高速でリクエストとキャンセルを繰り返すため、全く別のリクエストと誤認され、プロンプトキャッシュが機能せずに著しいパフォーマンス低下を引き起こします。

  - **さらに"-b 512"オプションも指定してください。**
    RTX 4070などの一般的なNVIDIAゲーミングGPUでは、512を超えてもパフォーマンスはほぼ変わりません。llama-serverのキャンセルリクエストはバッチ処理中には受理されないため、デフォルトの2048だとキャンセル実行までに数秒かかる場合があり、想定外のレスポンス低下を引き起こします。

## プライバシーとテレメトリー
- Cotabは、デフォルトのエンドポイント`"http://localhost:8080/v1"`またはユーザーが指定したLLM APIとのみ通信します。他の外部サービスやサーバーには接続しません。これにより、最大限のプライバシーとセキュリティが確保されます。
  - 設定されたAPIとのみ通信
  - テレメトリーや使用データは一切送信されません
  - ユーザーのコードや入力が第三者と共有されることはありません
  - 個人情報は収集または保存されません
  - このプロジェクトはオープンソースであり、すべてのソースコードがGitHubで利用可能です

- このポリシーにより、Cotabを完全に安心して使用できます。
- 注意：ローカルサーバーをインストールする場合、[llama.cpp githubリポジトリ](https://github.com/ggml-org/llama.cpp/releases)にアクセスします。

## コミュニティ & フィードバック

💬 質問、アイデア、使用方法の議論は  
[GitHub Discussions](https://github.com/cotab-org/cotab/discussions)  
で歓迎します（英語 / 日本語OK）

🐞 バグを見つけた場合は、代わりにIssueを作成してください。

## 開発 / 貢献

- 貢献（イシュー、PR、改善提案）を歓迎します。
- バグ修正、最適化、ベンチマーク結果の共有も歓迎します。

## ビルド方法

- セットアップ要件

  事前にVS Codeをインストールしてください。

- Windows

  この単一コマンドを実行すると、セットアップスクリプトが自動的にダウンロードされ実行されます。GitやNode.jsを含む何も必要ありません - すべてのポータブル版が自動的にダウンロードされ、./workspaceにセットアップされ、プロジェクトがクローンされ、VS Codeが起動します:
  
  ```bash
  mkdir cotab
  cd cotab
  powershell -NoProfile -Command "$f='run-vscode.bat'; (New-Object Net.WebClient).DownloadString('https://github.com/cotab-org/cotab/raw/refs/heads/main/run-vscode.bat') -replace \"`r?`n\",\"`r`n\" | Set-Content $f -Encoding ASCII; cmd /c $f"
  ```
  
  vscodeでF5を押してプラグインのデバッグを開始します。
  
- Ubuntu

  Node.js(v22)が必要です。
    
  例：パッケージマネージャー経由でNode.js v22をインストール。
  ```bash
  curl -fsSL https://deb.nodesource.com/setup_22.x | sudo -E bash -
  sudo apt install -y nodejs
  ```
  
  Cotabのクローンと設定。
  
  ```bash
  git clone https://github.com/cotab-org/cotab.git
  cd cotab
  npm install
  code .\
  ```
  
  vscodeでF5を押してプラグインのデバッグを開始します。
  
- MacOS

  Node.js(v22)が必要です。
  
  例：macosでNode.js v22をインストール。
  ```bash
  # nvmをインストール
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
  
  # nvmを有効化
  \. "$HOME/.nvm/nvm.sh"
  
  # node.js v22をインストール
  nvm install 22
  node -v
  ```
  
  Cotabのクローンと設定。
  
  ```bash
  git clone https://github.com/cotab-org/cotab.git
  cd cotab
  npm install
  code .\
  ```
  
  vscodeでF5を押してプラグインのデバッグを開始します。

- パッケージの作成

  ```bash
  npx vsce package
  ```

## FAQ

### Cotabの使用を開始する際に、ウィンドウが一瞬ちらつくのはなぜですか？
一瞬のウィンドウのちらつきは、Cotabが初期化中にフォントサイズを計算するために発生します。VS Codeは文字サイズを取得する直接的なAPIを提供していないため、CotabはWebviewを使用してフォントサイズを計算します。これにより、Cotabの使用を開始する際に一瞬のちらつきが発生します。

### モデル名にある4Bや30Bとは何ですか？
パラメータ数（パラメータの数）を表しています。Bは「Billion」（10億）を意味します。例えば、4Bは40億パラメータ、30Bは300億パラメータを意味します。一般的に、パラメータ数が多いほどモデルの性能は向上しますが、必要なメモリや計算リソースも増加します。

### モデル名にあるA3Bとは何ですか？
実際の演算量が3B（30億パラメータ）相当であることを表しています。Aは「Active」（アクティブ）を意味します。例えば、Qwen3-Coder-30B-A3Bは300億パラメータのモデルですが、実際の推論時の演算量は3B相当に最適化されています。これにより、30Bモデルの高品質な性能を保ちながら、3Bモデルと同程度の高速な推論が可能になります。

### モデル名にあるQ4やQ2とは何ですか？
量子化ビット数を表しています。量子化とは、モデルの精度を下げることでファイルサイズを小さくし、メモリ使用量を削減する技術です。Q4は4ビット量子化、Q2は2ビット量子化を意味します。数値が小さいほどファイルサイズとメモリ使用量は少なくなりますが、モデルの品質は低下します。一般的に、Q4モデルはデータサイズと品質のバランスが優れているといわれています。

### Qwen3-Coder-30B-A3BのQ2での品質劣化はどの程度ですか？
Qwen3-Coder-30B-A3B:Q2の場合、構文が頻繁に崩れるなどの重大な劣化までは発生しません。パフォーマンスと精度の観点から、少ないVRAM環境において優位性があるため、実際に使用して試してみることをお勧めします。

### 補完が見切れたり、既存のコードの覆いかぶさらないようにできない？
VS Codeが公開しているAPIでは、Cursor等のように編集ビューをはみ出した表示をすることができません。また、行の間に任意のブロックを追加することもできません。GitHub Copilotは公開されていない内部機能を使って実現しているため、通常のプラグインでGitHub Copilotと同等のUXを実装することができません。

### トークンとは何ですか？
トークンは、LLMがテキストを処理する際の基本単位です。例えば「Hello」は約1トークンです。コードの場合、トークン数はおおよそ文字数の1/3程度になります。


## ライセンス
Copyright (c) 2025-2026 cotab
Apache License 2.0

