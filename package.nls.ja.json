{
  "description": "このVS Code拡張機能は、完全にローカルLLM上で動作することで、最大限のプライバシーとセキュリティを考慮して設計されたAI駆動のマルチライン自動補完プラグインです。",
  "command.toggleEnabled": "Cotab: 有効/無効",
  "command.openSettingsEnable": "Cotab: 有効設定を開く",
  "command.cancelSuggestions": "Cotab: 進行中の生成をキャンセル",
  "command.acceptSuggestion": "Cotab: 現在の提案を受け入れる（すべての行）",
  "command.acceptFirstLineSuggestion": "Cotab: 現在の提案を受け入れる（最初の行）",
  "command.clearSuggestions": "Cotab: すべての提案をクリア",
  "command.server.install": "Cotab: [サーバー] インストールまたは更新 - Web経由でシステムにllama.cppを簡単にインストール/更新します。",
  "command.server.uninstall": "Cotab: [サーバー] アンインストール - システムにインストールされたllama.cppをアンインストールします",
  "command.server.start": "Cotab: [サーバー] 起動 - システム上のllama-serverを起動します",
  "command.server.stop": "Cotab: [サーバー] 停止 - システム上のllama-serverを停止します",
  "command.menu.clickstatusbar": "Cotab: [メニュー] クリックせずにホバーすると、メニューが表示されます！",
  "config.basic.enabled": "自動補完を有効/無効にする",
  "config.basic.disableForExtensions": "拡張子に対して自動補完を無効にする（ファイル拡張子のカンマ区切りリスト。例: 'txt,json'）",
  "config.basic.autoStart": "拡張機能がアクティブ化されたときにサーバーを自動的に起動します。cotab.llm.apiBaseURL設定は空白である必要があります。",
  "config.basic.autoStopOnIdleTime": "サーバーを自動停止するまでのアイドルタイムアウト（秒）",
  "config.basic.commentLanguage": "コメント言語。空の場合、OS言語を使用します（例: 'English', '日本語', '简体中文', 'Français'）",
  "config.basic.selectedPromptMode": "使用するプロンプト設定モード。空の場合、デフォルトのコーディングプロンプトを使用します",
  "config.llm.llamaCppVersion": "インストールされているllama.cpp（llama-server）のバージョン。（Custom: customLlamaCppVersion）",
  "config.llm.customLlamaCppVersion": "llama.cpp（llama-server）のカスタムバージョン。'b7010'など（llamaCppVersionがCustomの場合のみ使用）",
  "config.llm.provider": "使用するLLMプロバイダー",
  "config.llm.apiBaseURL": "OpenAI互換のベースURL（例: `http://localhost:8080/v1`）推奨: llama-server（ローカルサーバーの自動起動を使用する場合は空白のままにしてください。）",
  "config.llm.apiKey": "OpenAI互換サーバー用のBearerトークンとして使用されるAPIキー（オプション。空白のままにできます。）",
  "config.llm.localServerPreset": "ローカルサーバー（llama.cpp）のプリセット引数。VRAM使用量の見積もりはコンテキストサイズ32kの場合です。",
  "config.llm.localServerPreset.enum.0": "モデル:Qwen3-4B-Instruct-2507\n\n-hf unsloth/Qwen3-4B-Instruct-2507-GGUF --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0.01 --repeat-penalty 1.05 --jinja -fa on -ngl 999 -kvu -ctk q8_0 -ctv q8_0",
  "config.llm.localServerPreset.enum.1": "モデル:Qwen3-Coder-30B-A3B-Instruct-UD-IQ2_M\n\n-hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Qwen3-Coder-30B-A3B-Instruct-UD-IQ2_M --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0.01 --repeat-penalty 1.05 --jinja -fa on -ngl 999 -kvu --n-cpu-moe 35",
  "config.llm.localServerPreset.enum.2": "モデル:Qwen3-Coder-30B-A3B-Instruct-UD-IQ2_M\n\n-hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Qwen3-Coder-30B-A3B-Instruct-UD-IQ2_M --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0.01 --repeat-penalty 1.05 --jinja -fa on -ngl 999 -kvu --n-cpu-moe 11",
  "config.llm.localServerPreset.enum.3": "モデル:Qwen3-Coder-30B-A3B-Instruct-UD-IQ2_M\n\n-hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Qwen3-Coder-30B-A3B-Instruct-UD-IQ2_M --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0.01 --repeat-penalty 1.05 --jinja -fa on -ngl 999 -kvu",
  "config.llm.localServerPreset.enum.4": "モデル:Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL\n\n-hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0.01 --repeat-penalty 1.05 --jinja -fa on -ngl 999 -kvu",
  "config.llm.localServerPreset.enum.5": "カスタム設定にはcotab.llm.localServerCustomを使用",
  "config.llm.localServerCustom": "ローカルサーバー（llama.cpp）を起動するための引数",
  "config.llm.localServerPort": "ローカルサーバー（llama.cpp）のポート",
  "config.llm.localServerContextSize": "ローカルサーバー（llama.cpp）のコンテキストサイズ",
  "config.llm.localServerCacheRam": "ローカルサーバー（llama.cpp）の最大RAMバックアップキャッシュサイズ（MiB）（-1 - 制限なし、0 - 無効） https://github.com/ggml-org/llama.cpp/pull/16391",
  "config.llm.model": "使用するリモートモデル名",
  "config.llm.temperature": "創造性（0-1、低い値が推奨。 -1の場合、サーバーのデフォルト設定が使用されます）",
  "config.llm.top_p": "Top-pサンプリング（-1の場合、サーバーのデフォルト設定が使用されます）",
  "config.llm.top_k": "Top-kサンプリング（-1の場合、サーバーのデフォルト設定が使用されます）",
  "config.llm.maxTokens": "LLM出力の最大トークン数",
  "config.llm.maxOutputLines": "LLMからの最大出力行数",
  "config.llm.timeoutMs": "LLM呼び出しタイムアウト（ミリ秒）",
  "config.prompt.additionalSystemPrompt": "デフォルトのシステムプロンプトに追加する追加プロンプト",
  "config.prompt.additionalUserPrompt": "デフォルトのユーザープロンプトに追加する追加プロンプト",
  "config.prompt.additionalAssistantThinkPrompt": "デフォルトのアシスタント思考プロンプトに追加する追加プロンプト",
  "config.prompt.additionalAssistantOutputPrompt": "デフォルトのアシスタント出力プロンプトに追加する追加プロンプト",
  "config.promptDetail.startEditingHereSymbol": "編集の開始を示すシンボル文字列",
  "config.promptDetail.stopEditingHereSymbol": "編集の終了を示すシンボル文字列",
  "config.promptDetail.completeHereSymbol": "コード補完中に挿入されるシンボル文字列",
  "config.promptDetail.aroundBeforeLines": "カーソル位置の前で取得する行数（LLM推論ターゲット範囲）\n推奨: 0\n（0に設定すると、LLM出力がカーソル行から開始され、LLM出力が安定します）",
  "config.promptDetail.aroundAfterLines": "カーソル位置の後で取得する行数（編集ターゲット範囲）",
  "config.promptDetail.aroundMergeAfterLines": "マージ中に使用される周辺コードの終了行（編集ターゲット範囲）\n推奨: cotab.promptDetail.aroundAfterLines + LLM出力行（約10行）+ 約5行",
  "config.promptDetail.aroundCacheBeforeLines": "キャッシュ利用のための周辺コードの開始行。\nカーソル位置がこの行を超えない場合、ユーザープロンプト内のソースコードは更新されず、効果的なプロンプトキャッシュの使用が可能になります。",
  "config.promptDetail.aroundCacheAfterLines": "キャッシュ利用のための周辺コードの終了行。\nカーソル位置がこの行を超えない場合、ユーザープロンプト内のソースコードは更新されず、効果的なプロンプトキャッシュの使用が可能になります。",
  "config.promptDetail.aroundLatestAddBeforeLines": "最新利用のための周辺コードの開始行。\n最終的な最新行はaroundBeforeLines + aroundLatestAddBeforeLinesです",
  "config.promptDetail.aroundLatestAddAfterLines": "最新利用のための周辺コードの終了行。\n最終的な最新行はaroundAfterLines + aroundLatestAddAfterLinesです",
  "config.promptDetail.maxSymbolCharNum": "シンボルコードブロックに含めるシンボルの最大文字数。\n約1000文字で約20個のシンボル入力が可能です。\nQwen3:4b-Instruct-2507では、約1000文字で約250トークンを使用します。",
  "config.promptDetail.enableCodeSummary": "ソースコード要約機能を有効にする。\n有効にすると、ソースコードが要約され、プロンプトに含まれます。",
  "config.detail.logLevel": "ログ出力レベル（ERROR: エラーのみ、WARNING: 警告以上、INFO: 情報以上、DEBUG: すべて）",
  "config.ui.showProgressSpinner": "コード補完中にスピナーアイコンを表示",
  "config.gettingStarted.hideOnStartup": "次回起動時にGetting Startedビューを表示しない"
}

