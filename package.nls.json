{
  "description": "This VS Code extension is an AI-powered multi-line autocomplete plugin designed with maximum privacy and security in mind by running entirely on a local LLM.",
  "command.toggleEnabled": "Cotab: Enable/Disable",
  "command.openSettingsEnable": "Cotab: Open Enable Setting",
  "command.cancelSuggestions": "Cotab: Cancel In-Progress Generation",
  "command.acceptSuggestion": "Cotab: Accept Current Suggestion (All Lines)",
  "command.acceptFirstLineSuggestion": "Cotab: Accept Current Suggestion (First Line)",
  "command.clearSuggestions": "Cotab: Clear All Suggestions",
  "command.server.install": "Cotab: [Server] Install or update - Easily install/update llama.cpp on the system via the web.",
  "command.server.uninstall": "Cotab: [Server] Uninstall - Uninstall llama.cpp installed on the system",
  "command.server.start": "Cotab: [Server] Start - Start the llama-server on the system",
  "command.server.stop": "Cotab: [Server] Stop - Stop the llama-server on the system",
  "command.menu.clickstatusbar": "Cotab: [Menu] If you hover without clicking, the menu will appear!",
  "config.basic.enabled": "Enable/disable autocomplete",
  "config.basic.nextEditJump": "Enable/disable next edit jump",
  "config.basic.disableForExtensions": "Disable autocomplete for extensions (comma separated list of file extensions. e.g. 'txt,json')",
  "config.basic.autoStart": "Automatically start server when extension activates. The cotab.llm.apiBaseURL setting must be blank.",
  "config.basic.autoStopOnIdleTime": "Idle timeout in seconds before auto-stopping server",
  "config.basic.commentLanguage": "Comment language. If empty, uses OS language (e.g. 'English', '日本語', '简体中文', 'Français')",
  "config.basic.selectedPromptMode": "Used prompt settings mode. If empty, uses default coding prompt",
  "config.llm.llamaCppVersion": "Installed version of llama.cpp (llama-server). (Custom: customLlamaCppVersion)",
  "config.llm.customLlamaCppVersion": "Custom version of llama.cpp (llama-server). like 'b7010' (only used when llamaCppVersion is Custom)",
  "config.llm.provider": "LLM provider to use",
  "config.llm.apiBaseURL": "OpenAI compatible Base URL (e.g. `http://localhost:8080/v1`) recommended llama-server (If you use auto-start the Local Server, leave it blank.)",
  "config.llm.apiKey": "API key used as Bearer token for OpenAI compatible servers (Optional. You can leave this blank.)",
  "config.llm.localServerPreset": "Preset arguments for local server (llama.cpp). VRAM usage estimate is for context size 32k.",
  "config.llm.localServerCustom": "Arguments for launching the local server (llama.cpp)",
  "config.llm.localServerPort": "Port for the local server (llama.cpp)",
  "config.llm.localServerContextSize": "Context size for the local server (llama.cpp)",
  "config.llm.localServerCacheRam": "the maximum ram backup cache size in MiB (-1 - no limit, 0 - disable) for the local server (llama.cpp) https://github.com/ggml-org/llama.cpp/pull/16391",
  "config.llm.model": "Remote model name to use",
  "config.llm.temperature": "Creativity (0-1, lower values recommended. if -1, server's default setting is used)",
  "config.llm.top_p": "Top-p sampling (if -1, server's default setting is used)",
  "config.llm.top_k": "Top-k sampling (if -1, server's default setting is used)",
  "config.llm.maxTokens": "Maximum number of tokens for LLM output",
  "config.llm.maxOutputLines": "Maximum number of output lines from LLM",
  "config.llm.timeoutMs": "LLM call timeout (ms)",
  "config.prompt.additionalSystemPrompt": "Additional prompt to add to the default system prompt",
  "config.prompt.additionalUserPrompt": "Additional prompt to add to the default user prompt",
  "config.prompt.additionalAssistantThinkPrompt": "Additional prompt to add to the default assistant thinking prompt",
  "config.prompt.additionalAssistantOutputPrompt": "Additional prompt to add to the default assistant output prompt",
  "config.promptDetail.startEditingHereSymbol": "Symbol string indicating the start of editing",
  "config.promptDetail.stopEditingHereSymbol": "Symbol string indicating the end of editing",
  "config.promptDetail.completeHereSymbol": "Symbol string inserted during code completion",
  "config.promptDetail.aroundBeforeLines": "Number of lines to retrieve before cursor position (LLM inference target range)\n Recommended: 0 \n(Setting to 0 makes LLM output start from cursor line, stabilizing LLM output)",
  "config.promptDetail.aroundAfterLines": "Number of lines to retrieve after cursor position (edit target range)",
  "config.promptDetail.aroundMergeAfterLines": "End line of surrounding code used during merge (edit target range)\nRecommended: cotab.promptDetail.aroundAfterLines + LLM output lines (~10) + about 5 lines",
  "config.promptDetail.aroundCacheBeforeLines": "Start line of surrounding code for cache utilization.\nIf cursor position doesn't exceed this line, source code in User Prompt won't be updated, enabling effective Prompt Cache usage.",
  "config.promptDetail.aroundCacheAfterLines": "End line of surrounding code for cache utilization.\nIf cursor position doesn't exceed this line, source code in User Prompt won't be updated, enabling effective Prompt Cache usage.",
  "config.promptDetail.aroundLatestAddBeforeLines": "Start line of surrounding code for latest utilization.\nfinal latest lins is aroundBeforeLines + aroundLatestAddBeforeLines",
  "config.promptDetail.aroundLatestAddAfterLines": "End line of surrounding code for latest utilization.\nfinal latest lins is aroundAfterLines + aroundLatestAddAfterLines",
  "config.promptDetail.maxSymbolCharNum": "Maximum number of characters for symbols to include in symbol code blocks.\nApproximately 1000 characters allow for 20 symbol inputs.\nIn Qwen3:4b-Instruct-2507, approximately 1000 characters use about 250 tokens.",
  "config.promptDetail.enableCodeSummary": "Enable source code summary feature.\nIf enabled, the source code will be summarized and included in the prompt.",
  "config.detail.logLevel": "Log output level (ERROR: errors only, WARNING: warnings and above, INFO: info and above, DEBUG: everything)",
  "config.ui.showProgressSpinner": "Show spinner icon while completing code completion",
  "config.gettingStarted.hideOnStartup": "Do not show Getting Started view on next startup",
  "config.gettingStarted.showAllPresets": "Show all presets in the Getting Started view (including hidden presets)"
}

