{
  "gettingStarted.title": "Cotab Getting Started",
  "gettingStarted.gettingStarted": "Getting Started",
  "gettingStarted.setupServer": "Setup Server",
  "gettingStarted.preset": "Preset",
  "gettingStarted.showAllPresets": "Show All Presets",
  "gettingStarted.customArgsPlaceholder": "Enter custom llama-server arguments",
  "gettingStarted.contextSize": "Context Size",
  "gettingStarted.contextSizeTooltip": "Required:\n - set 16k (16384) or more.\n\nRecommended:\n - set 32k (32768) or more.\n\nReason:\n - The system prompt uses about 5k, and 1,000 lines of code use about 12k more.\n - so please set the context window to 20k (20480) or more.\n\nThe default model (Qwen3-4B-Instruct-2507) VRAM usage:\n - 16k: about 4 GB\n - 32k: about 5.5 GB",
  "gettingStarted.or": "OR",
  "gettingStarted.apiBaseURL": "OpenAI compatible Base URL",
  "gettingStarted.apiBaseURLTooltip": "If you use auto-start the Local Server, leave it blank.",
  "gettingStarted.apiKey": "API Key",
  "gettingStarted.model": "Model",
  "gettingStarted.letsGetAutocompleting": "Let's Get Autocompleting!",
  "gettingStarted.command": "Command",
  "gettingStarted.keybinding": "Keybinding",
  "gettingStarted.acceptAll": "Accept All",
  "gettingStarted.acceptFirstLine": "Accept First Line",
  "gettingStarted.reject": "Reject",
  "gettingStarted.rejectNote": "Note: By rejecting, you can change the next completion candidates.",
  "gettingStarted.showThisPageAgain": "Show This Page Again",
  "gettingStarted.hoverStatusBar": "Hover over the status bar. Don't click!",
  "gettingStarted.dontShowAgain": "Don't show this again",
  "gettingStarted.learnMore": "Learn More",
  "gettingStarted.progressIconDescription": "Progress Icon Description",
  "gettingStarted.analyzing": "Analyzing",
  "gettingStarted.completingCurrentLine": "Completing<br>current line",
  "gettingStarted.completingAfterCurrentLine": "Completing<br>after current line",
  "gettingStarted.showProgressIcon": "Show progress icon",
  "gettingStarted.detailSettings": "Detail Settings",
  "gettingStarted.commentLanguage": "Comment Language",
  "gettingStarted.commentLanguageExample": "(e.g. 'English', '日本語', '简体中文', 'Français')",
  "gettingStarted.openSettings": "Open Settings",
  "gettingStarted.openSettingsTooltip": "Open Cotab Settings",
  "gettingStarted.server.stop": "Stop Server",
  "gettingStarted.server.network": "Network Server Connected",
  "gettingStarted.server.start": "Start Server",
  "gettingStarted.server.install": "Install Server",
  "gettingStarted.server.unsupported": "Install Not Supported",
  "gettingStarted.server.autoStartTooltip": "If you use auto-start, the \"OpenAI compatible Base URL\" setting must be blank.",
  "gettingStarted.saveError": "Failed to save setting: {0}",
  "menuIndicator.codeCompletions": "Code Completions",
  "menuIndicator.enableGlobally": "Enable globally",
  "menuIndicator.enableFor": "Enable for {0}",
  "menuIndicator.installingServer": "Installing Server (llama.cpp) ...",
  "menuIndicator.serverAlreadyInstalled": "Server (llama.cpp) is already installed. Do you want to update?",
  "menuIndicator.yes": "Yes",
  "menuIndicator.no": "No",
  "menuIndicator.serverInstallFailed": "Server (llama.cpp) install failed: {0}",
  "menuIndicator.serverNotInstalled": "not installed Server (llama.cpp)",
  "menuIndicator.reallyUninstallServer": "Really uninstall the server (llama.cpp)?",
  "menuIndicator.uninstallingServer": "Uninstalling Server (llama.cpp) ...",
  "menuIndicator.serverUninstalled": "Uninstalled Server (llama.cpp)",
  "menuIndicator.serverUninstallFailed": "Server (llama.cpp) uninstall failed: {0}",
  "menuIndicator.serverAlreadyRunning": "Local server is already running. Do you want to restart it?",
  "menuIndicator.startServer": "Start llama-server",
  "menuIndicator.stopServer": "Stop llama-server",
  "menuIndicator.promptMode.Coding": "Coding",
  "menuIndicator.promptMode.Comment": "Comment",
  "menuIndicator.promptMode.Translate": "Translate",
  "menuIndicator.promptMode.Proofreading(experimental)": "Proofreading(experimental)",
  "menuIndicator.promptMode.BusinessChat(experimental)": "BusinessChat(experimental)",
  "terminalCommand.installNotSupported": "Automatic install/upgrade is supported only for Mac and Windows for now. Download llama.cpp package manually and add the folder to the path. Visit github.com/ggml-org/llama.vscode/wiki for details.",
  "terminalCommand.uninstallNotSupported": "Automatic uninstall is supported only for Mac and Windows for now. Please uninstall llama.cpp manually. Visit github.com/ggml-org/llama.vscode/wiki for details."
}

